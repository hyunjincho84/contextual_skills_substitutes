# -*- coding: utf-8 -*-
import os, json, random, glob, math
from typing import List

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer
from tqdm import tqdm

from model import BERTForSkillPrediction


# â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_ROOT      = "/home/jovyan/LEM_data2/hyunjincho/preprocessed_www_new"
TRAIN_DIR      = os.path.join(DATA_ROOT, "train")
VOCAB_PATH     = os.path.join(DATA_ROOT, "skill2idx.json")

MODEL_NAME     = "/home/jovyan/LEM_data2/hyunjincho/bert_pretrained/checkpoint-165687"
CKPT_DIR       = "/home/jovyan/LEM_data2/hyunjincho/checkpoints(www)_new"
BEST_MODEL_PT  = os.path.join(CKPT_DIR, "best_model.pt")

OUT_DIR        = "/home/jovyan/LEM_data2/hyunjincho/sae_layerwise_out_8192"

MAX_LEN        = 512
BATCH_SIZE     = 256
NUM_WORKERS    = 8
PIN_MEMORY     = True
SEED           = 42
DEVICE         = "cuda"

# SAE hyperparams
SAE_HIDDEN     = 4096
SAE_EPOCHS     = 3
SAE_LR         = 1e-4
L1             = 1e-3
L2             = 1e-4

USE_AMP        = True
AMP_DTYPE      = torch.bfloat16   # A100 ìµœì 

TEXT_COL       = "masked_sentence"


# â”€â”€â”€ Global CUDA tuning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True


# â”€â”€â”€ Utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def iter_train_files(train_dir: str) -> List[str]:
    return sorted(glob.glob(os.path.join(train_dir, "[0-9]"*4, "preprocessed_*.csv.gz")))


# â”€â”€â”€ Dataset (masked_sentence ì „ì²´ ì‚¬ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class MaskedSentenceDataset(Dataset):
    """
    - iterrows ì œê±°
    - pandas vectorized loading
    - [MASK] í¬í•¨ ì—¬ë¶€ë¡œ í•„í„°ë§í•˜ì§€ ì•Šê³ , masked_sentence ì»¬ëŸ¼ ì „ì²´ ì‚¬ìš©
      (ì „ì œ: ì´ ì»¬ëŸ¼ì€ ì´ë¯¸ ë§ˆìŠ¤í‚¹ëœ ë¬¸ì¥ë“¤ë¡œ êµ¬ì„±)
    """
    def __init__(self, csv_path: str):
        df = pd.read_csv(csv_path, compression="gzip", usecols=[TEXT_COL])
        self.sentences = df[TEXT_COL].dropna().astype(str).tolist()

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        return self.sentences[idx]


# â”€â”€â”€ Collate: batch-tokenize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_collate_fn(tokenizer: BertTokenizer):
    def collate(batch):
        # batchëŠ” sentence(str) ë¦¬ìŠ¤íŠ¸
        enc = tokenizer(
            batch,
            padding="max_length",
            truncation=True,
            max_length=MAX_LEN,
            return_tensors="pt",
        )
        input_ids = enc["input_ids"]           # (B, T)
        attention_mask = enc["attention_mask"] # (B, T)

        # ì²« ë²ˆì§¸ [MASK] ìœ„ì¹˜. (masked_sentenceë¼ë©´ í•­ìƒ ì¡´ì¬í•œë‹¤ê³  ê°€ì •)
        mask_pos = (input_ids == tokenizer.mask_token_id)
        has_mask = mask_pos.any(dim=1)

        # ë°©ì–´: í˜¹ì‹œ ë§ˆìŠ¤í¬ê°€ ì—†ëŠ” ìƒ˜í”Œì´ ì„ì—¬ ìˆìœ¼ë©´ ì œê±°
        if not bool(has_mask.all()):
            keep = has_mask.nonzero(as_tuple=True)[0]
            input_ids = input_ids[keep]
            attention_mask = attention_mask[keep]
            mask_pos = mask_pos[keep]

        mask_idx = mask_pos.float().argmax(dim=1)  # (B,)
        return input_ids, attention_mask, mask_idx
    return collate


# â”€â”€â”€ SAE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SparseAutoencoder(nn.Module):
    def __init__(self, d_in: int, d_hidden: int):
        super().__init__()
        self.W_e = nn.Linear(d_in, d_hidden, bias=True)
        self.W_d = nn.Linear(d_hidden, d_in, bias=False)
        self.b_d = nn.Parameter(torch.zeros(d_in))

        nn.init.kaiming_uniform_(self.W_e.weight, a=math.sqrt(5))
        nn.init.zeros_(self.W_e.bias)
        nn.init.zeros_(self.W_d.weight)

    def encode(self, x):
        return F.relu(self.W_e(x - self.b_d))

    def forward(self, x):
        f = self.encode(x)
        xhat = self.W_d(f) + self.b_d
        return f, xhat


def sae_loss(x, xhat, f):
    return (
        F.mse_loss(xhat, x)
        + L1 * f.abs().mean()
        + L2 * f.pow(2).mean()
    )


# â”€â”€â”€ Efficient BERT forward (layerê¹Œì§€ë§Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@torch.no_grad()
def forward_until_layer(bert, input_ids, attention_mask, layer: int):
    """
    layer: 1..12
    BERT embeddings + encoder.layer[0:layer]ê¹Œì§€ë§Œ í†µê³¼.
    """
    device = input_ids.device
    x = bert.embeddings(input_ids)

    # âš ï¸ device ì¸ì ì—†ì´ í˜¸ì¶œ (FutureWarning ë°©ì§€)
    ext_mask = bert.get_extended_attention_mask(
        attention_mask, attention_mask.shape
    )

    # attention maskë¥¼ í˜„ì¬ deviceë¡œ ë³´ì¥
    ext_mask = ext_mask.to(device)

    for i in range(layer):
        x = bert.encoder.layer[i](x, ext_mask)[0]

    return x  # (B, T, H)


# â”€â”€â”€ Train SAE per layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_sae_one_layer(layer: int, bert, tokenizer, files: List[str]):

    layer_dir = os.path.join(OUT_DIR, f"layer_{layer:02d}")
    ensure_dir(layer_dir)

    sae = SparseAutoencoder(
        d_in=bert.config.hidden_size,
        d_hidden=SAE_HIDDEN
    ).to(DEVICE)

    opt = torch.optim.AdamW(sae.parameters(), lr=SAE_LR, fused=True)

    best_loss = float("inf")
    collate_fn = make_collate_fn(tokenizer)

    for ep in range(SAE_EPOCHS):
        sae.train()
        random.shuffle(files)
        total_loss, total_n = 0.0, 0

        print(f"\n[SAE] Layer {layer:02d} | Epoch {ep+1}/{SAE_EPOCHS}")

        for fp in tqdm(files, desc="Files", unit="file"):
            ds = MaskedSentenceDataset(fp)
            if len(ds) == 0:
                continue

            loader = DataLoader(
                ds,
                batch_size=BATCH_SIZE,
                shuffle=True,
                num_workers=NUM_WORKERS,
                pin_memory=PIN_MEMORY,
                persistent_workers=True,
                prefetch_factor=4,
                collate_fn=collate_fn,
                drop_last=False,
            )

            for input_ids, attn_mask, mask_idx in loader:
                if input_ids.numel() == 0:
                    continue

                input_ids = input_ids.to(DEVICE, non_blocking=True)
                attn_mask = attn_mask.to(DEVICE, non_blocking=True)
                mask_idx  = mask_idx.to(DEVICE, non_blocking=True)

                with torch.autocast("cuda", dtype=AMP_DTYPE, enabled=USE_AMP):
                    h = forward_until_layer(bert, input_ids, attn_mask, layer)
                    x = h[torch.arange(h.size(0), device=DEVICE), mask_idx]  # (B, H)
                    f, xhat = sae(x)
                    loss = sae_loss(x, xhat, f)

                opt.zero_grad(set_to_none=True)
                loss.backward()
                opt.step()

                bs = x.size(0)
                total_loss += float(loss.item()) * bs
                total_n += bs

        avg_loss = total_loss / max(1, total_n)
        print(f"avg_loss={avg_loss:.6f} | samples={total_n:,}")

        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(sae.state_dict(), os.path.join(layer_dir, "sae_best.pt"))
            print(f"ğŸ† saved best (loss={best_loss:.6f})")

    torch.save(sae.state_dict(), os.path.join(layer_dir, "sae_last.pt"))
    print("ğŸ’¾ saved last")


# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    ensure_dir(OUT_DIR)
    set_seed(SEED)

    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

    with open(VOCAB_PATH) as f:
        skill2idx = json.load(f)

    model = BERTForSkillPrediction(MODEL_NAME, num_skills=len(skill2idx)).to(DEVICE)
    model.load_state_dict(torch.load(BEST_MODEL_PT, map_location="cpu"))
    model.eval()

    bert = model.bert.to(DEVICE)
    bert.eval()
    for p in bert.parameters():
        p.requires_grad_(False)

    files = iter_train_files(TRAIN_DIR)
    if not files:
        raise FileNotFoundError(f"No train files found under: {TRAIN_DIR}")

    for layer in range(1, 13):
        train_sae_one_layer(layer, bert, tokenizer, files)

    print("ğŸ‰ DONE")


if __name__ == "__main__":
    main()