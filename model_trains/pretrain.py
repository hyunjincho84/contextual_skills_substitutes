import os
import re
import gzip
import random
import pandas as pd
from pathlib import Path
from typing import List, Dict

from torch.utils.data import IterableDataset
from transformers import (
    BertTokenizer, BertForMaskedLM,
    DataCollatorForLanguageModeling,
    TrainingArguments, Trainer
)
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œìš©

# â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR   = "/home/jovyan/LEM_data/us/csv/fortnightly/all/20250607"  # âœ… ì—°/ì›” ì „ì²´ê°€ ë“¤ì–´ìˆëŠ” ë£¨íŠ¸
MODEL_NAME = "bert-base-uncased"
OUTPUT_DIR = "/home/jovyan/LEM_data2/hyunjincho/bert_pretrained"
MAX_LEN    = 512
MLM_PROB   = 0.15
FILES_PER_MONTH = 1
SEED = 42  # âœ… ì¬í˜„ì„±

os.makedirs(OUTPUT_DIR, exist_ok=True)
random.seed(SEED)

# â”€â”€â”€ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

# â”€â”€â”€ ìŠ¤ëƒ…ìƒ· ë””ë ‰í† ë¦¬ì—ì„œ ì›”ë³„ë¡œ íŒŒì¼ 3ê°œ ìƒ˜í”Œë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SNAPSHOT_DIR_PATTERN = re.compile(r"all_for_(\d{4})-(\d{2})-01$")

def collect_monthly_files(base_dir: str, files_per_month: int, seed: int = 42) -> pd.DataFrame:
    """
    base_dir í•˜ìœ„ì˜ .../<YEAR>/all_for_YYYY-MM-01/ ë””ë ‰í† ë¦¬ë“¤ì„ ì°¾ì•„
    ê° ì›”ë³„ë¡œ .csv.gz íŒŒì¼ì„ 3ê°œ(ë¶€ì¡±í•˜ë©´ ìˆëŠ” ë§Œí¼) ìƒ˜í”Œë§í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜.
    """
    rows: List[Dict] = []

    for root, dirs, files in os.walk(base_dir):
        # ì›” ìŠ¤ëƒ…ìƒ· ë””ë ‰í† ë¦¬ë§Œ ì„ ë³„
        m = SNAPSHOT_DIR_PATTERN.search(os.path.basename(root))
        if not m:
            continue

        year, month = m.group(1), m.group(2)
        csv_gz_files = sorted([
            os.path.join(root, f)
            for f in files
            if f.endswith(".csv.gz")
        ])

        if len(csv_gz_files) == 0:
            continue

        # ì¬í˜„ ê°€ëŠ¥í•œ ìƒ˜í”Œë§
        local_rng = random.Random((hash(root) ^ seed) & 0xffffffff)
        if len(csv_gz_files) > files_per_month:
            sampled = local_rng.sample(csv_gz_files, files_per_month)
        else:
            sampled = csv_gz_files

        for fpath in sampled:
            rows.append({
                "year": year,
                "month": month,
                "snapshot_dir": root,
                "file_path": fpath
            })

    df = pd.DataFrame(rows).sort_values(by=["year", "month", "file_path"]).reset_index(drop=True)
    return df

used_files_df = collect_monthly_files(BASE_DIR, FILES_PER_MONTH, SEED)
if used_files_df.empty:
    raise RuntimeError(f"No CSV.GZ files found under monthly snapshots in: {BASE_DIR}")

# ì‚¬ìš© íŒŒì¼ ëª©ë¡ ì €ì¥
used_files_csv = os.path.join(OUTPUT_DIR, "used_files.csv")
used_files_df.to_csv(used_files_csv, index=False)
print(f"âœ… Using {len(used_files_df)} files across months. Saved list to: {used_files_csv}")

# â”€â”€â”€ IterableDataset ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class JobPostingIterableDataset(IterableDataset):
    def __init__(self, file_list_df: pd.DataFrame, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.max_len = max_len
        # DataFrame â†’ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        self.files = file_list_df["file_path"].tolist()

        # âœ… ì „ì²´ ìƒ˜í”Œ ìˆ˜ ë¯¸ë¦¬ ê³„ì‚°(ì„ íƒì´ì§€ë§Œ Trainerì— ë„ì›€)
        self.total_samples = 0
        for path in tqdm(self.files, desc="Counting rows", unit="file"):
            try:
                with gzip.open(path, "rt", encoding="utf-8") as f:
                    # bodyë§Œ ë¯¸ë¦¬ ì½ì–´ fast count
                    df = pd.read_csv(f, usecols=["body"])
                    self.total_samples += df["body"].dropna().shape[0]
            except Exception as e:
                print(f"â— Error counting rows in {path}: {e}")

    def __len__(self):
        return self.total_samples  # Trainerê°€ ìŠ¤í… ì¶”ì • ì‹œ ì‚¬ìš©

    def parse_file(self, path):
        print(f"ğŸ“„ Processing: {os.path.basename(path)}")
        try:
            with gzip.open(path, "rt", encoding="utf-8") as f:
                df = pd.read_csv(f)
                if "body" not in df.columns:
                    return
                bodies = df["body"].dropna().astype(str).tolist()
                for text in tqdm(bodies, desc=f"ğŸ“š {os.path.basename(path)}", unit="body", leave=False):
                    encoded = self.tokenizer(
                        text,
                        padding="max_length",
                        truncation=True,
                        max_length=self.max_len,
                        return_tensors="pt"
                    )
                    yield {k: v.squeeze(0) for k, v in encoded.items()}
        except Exception as e:
            print(f"â— Error reading {path}: {e}")

    def __iter__(self):
        for path in tqdm(self.files, desc="ğŸ“‚ File Progress", unit="file"):
            yield from self.parse_file(path)

# â”€â”€â”€ ë°ì´í„°ì…‹ ë° ì½œë ˆì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
dataset = JobPostingIterableDataset(used_files_df, tokenizer, MAX_LEN)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=True,
    mlm_probability=MLM_PROB
)

# â”€â”€â”€ ëª¨ë¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = BertForMaskedLM.from_pretrained(MODEL_NAME)

# â”€â”€â”€ í•™ìŠµ ì¸ì ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=64,  # âš ï¸ MAX_LEN=512ì´ë©´ GPU ë©”ëª¨ë¦¬ í¬ê²Œ í•„ìš”
    save_steps=1000,
    save_total_limit=2,
    logging_steps=1000,
    logging_dir=f"{OUTPUT_DIR}/logs",
    report_to=["none"],
)

# â”€â”€â”€ Trainer êµ¬ì„± ë° í•™ìŠµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset,
    data_collator=data_collator,
)

print("ğŸš€ MLM Pretraining ì‹œì‘...")
trainer.train()
print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR}")
print(f"ğŸ“ ì‚¬ìš©í•œ íŒŒì¼ ëª©ë¡: {used_files_csv}")